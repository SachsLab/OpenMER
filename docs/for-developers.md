## Repository Organization

* Library and Application code in the /open_mer folder
  * Entry points in /open_mer/scripts
* TODO: Unit tests in the /tests folder.
* Documentation in the /docs folder
* Scratch scripts in /scripts

## Maintaining the Documentation

You will need to install several Python packages to maintain the documentation.

* `pip install mkdocs mkdocstrings mknotebooks mkdocs-material Pygments`

The /docs/{top-level-section} folders contain a mix of .md and .ipynb documentation. The latter are converted to .md by the [mknotebooks plugin](https://github.com/greenape/mknotebooks/projects) during building.

Run `mkdocs gh-deploy` to build the documentation, commit to the `gh-deploy` branch, and push to GitHub. This will make the documentation available at https://SachsLab.github.io/OpenMER/

### Autogenerated Documentation

The /docs/open_mer folder can hold stubs to tell the [mkdocstrings plugin](https://github.com/mkdocstrings/mkdocstrings) to build the API documentation from the docstrings in the library code itself. Currently, this is empty. If any stubs are added then it's necessary to build the documentation from a Python environment that has the package installed.

A stub takes the form

```
# Title
::: open_mer.module.name
```

### Testing the documentation locally

* `mkdocs serve`

If you build the docs locally then you'll also get the /site directory, but this should be git ignored.

## Running the unit tests

TODO

## Interprocess Communication

This section is referring to communication among the applications within the OpenMER suite, including mysqld and ~8 Python applications. Communication to/from the data sources is out-of-scope in this section.

The applications all run independently of each other, but most of them work better in combination. To communicate information between applications we use [ZeroMQ](https://zeromq.org/).

| Publisher      | Port  | Topic              | Message                                                | Subscribers    |
|----------------|-------|--------------------|--------------------------------------------------------|----------------|
| ProcedureGUI   | 60001 | procedure_settings | json of settings-dicts "procedure" and ??              | FeaturesGUI    |
| Depth_Process  | 60002 | snippet_status     | (startup, notrecording, recording, accumulating, done) | ProcedureGUI   |
| SweepGUI       | 60003 | channel_select     | json with channel, range, highpass                     | FeaturesGUI    |
| FeaturesGUI    | 60004 | features           | refresh                                                | ProcedureGUI   |
| DepthGUI       | 60005 | ddu                | float of depth                                         | Depth_Process  |

We also have one LSL stream coming from the DepthGUI. Old version of the SERF Depth_Process might still be using it but they should be migrated. 

| Stream Name     | Stream Type | Content                 | Inlets                     |
|-----------------|-------------|-------------------------|----------------------------|
| electrode_depth | depth       | 1 float32 of elec depth | *old* Depth_Process (SERF) |

## Development Environment

### Blackrock Neuroport

When using Blackrock hardware, the following tools and SDKs are needed.

The Blackrock NSP has its own [NeuroPort Central Suite](https://www.blackrockmicro.com/technical-support/software-downloads/) to manage the configuration of the device and to store data. However, its data visualization capabilities are rather limited and not suited for DBS MER.

The NSP data stream is accessible via an open source API [CereLink](https://github.com/CerebusOSS/CereLink) which includes a Python interface called `cerebus.cbpy`. These are maintained by Sachs Lab member Chadwick Boulay. Most of our OpenMER software is written in Python and depends on `cerebus.cbpy` and a custom [cerebuswrapper](https://github.com/SachsLab/cerebuswrapper) to communicate with the NSP.

#### nPlayServer

For development, it is useful to playback previously recorded data, without need of hardware or patients.
nPlayServer emulates a Blackrock Neuroport almost perfectly -- just with a different ip address and it plays back an old data file instead of provides new data. See below for platform-specific instructions.

When using nPlayServer, you can follow the general [Usage Instructions](./usage-instructions.md) with one modification:
Modify the [DepthGUI settings](settings.md#depthguiini) to use "cbsdk playback" as its source. The depth value might not update until the file playback emits a change in depth.

**Windows**

* Run "C:\Program Files (x86)\Blackrock Microsystems\NeuroPort Windows Suite\runNPlayAndCentral.bat"
* Select a recording to play back
* Use Central's hardware configuration tool to enable continuous recording and spike extraction on the recorded channels.

**Nix**

* Blackrock does not distribute nPlayServer on macOS or Linux. However, it does exist. Contact Chad directly.
* Central is unavailable on these platforms. Use `pycbsdk` to quickly change the hardware configuration.

### Playback XDF file

If you have a correctly formatted file, it may be enough to use [XDFStreamer](https://github.com/labstreaminglayer/App-XDFStreamer).

TODO: More instructions needed.

### Dependencies

We assume you know how to work with conda / mamba environments and that you have a MySQL database server running and configured to your liking.

* Create an `openmer` conda environment.
* Install the Python packages from the [table](preparing-distribution.md#required-python-packages).
* Adapt the instructions at [Segmented Electrophys Recordings and Features Database (SERF)](https://github.com/cboulay/SERF) to prepare the database server for your development environment.

## Future goal - installable package

-[] Refactor this repo to exist only as a library
-[] Create a new repo for entry points, leveraging above library, create installer using [fman build system](https://build-system.fman.io/)
